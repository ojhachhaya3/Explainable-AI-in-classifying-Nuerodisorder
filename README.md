# Explainable-AI-in-classifying-Nuerodisorder
## Overview

Explainable AI (XAI) refers to a set of techniques and methods that make the behavior of machine learning models understandable to humans. In healthcare and neuroscience, where high-stakes decisions are involved, interpretability is essential — not just for accuracy, but also for clinical trust, regulatory acceptance, and knowledge discovery.
The primary objective is to develop an interpretable machine learning pipeline that:
Classifies EEG recordings into Healthy and Schizophrenic classes.
Uses statistical features extracted from EEG signals.
Applies XAI techniques (e.g., SHAP, PDP, ICE) to understand why the model makes its decisions — enabling trust, transparency, and clinical insight and visualize how features influence model predictions.

# Dataset
https://drive.google.com/drive/folders/16j8TwPh92DjikvzWOfhN37qARd6vTdPc?usp=drive_link
